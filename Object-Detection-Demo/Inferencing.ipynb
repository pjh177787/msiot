{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of an object detector \n",
    "In this notebook, you will need to implement an CNN based object detector to detect specific objects in given images.\n",
    "\n",
    "You will find the resources in folder 'src'. It contains the image set for training and testing, the groundtruth of the images, and a folder to place your parameters (weights).\n",
    "\n",
    "<img src=\"pics/0__1_.jpg\" alt=\"Biker\" />\n",
    "\n",
    "This task is divided into three sections. This notebook contains only the first section. \n",
    "\n",
    "- In the first section, you will load weights to a pre-defined neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the devices available on your machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10218289023578660664\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3146173644\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12726111134654272992\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything, we need to import the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 996 image-label pair in the entire set\n",
      "First entry looks like this:\n",
      "['src/images/932.jpg', [0.48984375, 0.5055555555555555, 0.0328125, 0.08333333333333333]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load groundtruch\n",
    "'''\n",
    "import json\n",
    "with open('src/groundtruth.txt', 'r') as file:\n",
    "    lines = json.load(file)\n",
    "\n",
    "print('There are %d image-label pair in the entire set' %(len(lines)))\n",
    "print('First entry looks like this:')\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to pick ramdomly from the data set to put aside a subset of images for testing. The model **should not** see this subset of images while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function converts the image into the input type.\n",
    "'''\n",
    "def load_input(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((320,160))\n",
    "    input_img = np.asarray(img).astype(np.float32)\n",
    "    input_img = (input_img/255 - 0.5)/0.25\n",
    "    return input_img[np.newaxis,:]\n",
    "\n",
    "'''\n",
    "This function draws the rectangle around the object\n",
    "'''\n",
    "def show_image(image, box):\n",
    "    img = Image.open(image)\n",
    "    \n",
    "    box_top_left = (int((box[0] - box[2]/2)*640), int((box[1] + box[3]/2)*360))\n",
    "    box_bot_right = (int((box[0] + box[2]/2)*640), int((box[1] - box[3]/2)*360))\n",
    "\n",
    "    draw = ImageDraw.ImageDraw(img)\n",
    "    draw.rectangle((box_top_left, box_bot_right), outline = (255,0,0))\n",
    "\n",
    "    plt.figure(1, figsize = (16, 9), dpi =300)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('src/params_sets_alt', 'rb') as fp:\n",
    "    weights_list = pickle.load(fp, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section 1\n",
    "First, a pre-defined keras model is given. Your task is to play with the training configurations and explore their effects on the training performance (speed, accuracy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 17:42:27.667509 30028 deprecation.py:506] From C:\\Users\\Aperture\\Anaconda3\\envs\\tf-gpu-pip\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#this is the model for the testing part\n",
    "model = tf.keras.models.Sequential([\n",
    "    #first dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False, input_shape=(160, 320, 3)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(48, (1, 1), padding='same', use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #maxpooling\n",
    "    layers.MaxPool2D(strides =(2, 2)),\n",
    "    #second dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(96,(1,1), padding='same', use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #maxpooling\n",
    "    layers.MaxPool2D(strides =(2, 2)),\n",
    "    #third dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(192, (1, 1), padding='same', use_bias=False,strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #maxpooling\n",
    "    layers.MaxPool2D(strides=(2, 2)),\n",
    "    #fourth dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(384, (1, 1), padding='same', use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #fifth dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(512, (1, 1), padding='same',use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #output\n",
    "    layers.Conv2D(10,(1,1), padding='same',use_bias=False,strides=(1, 1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "depthwise_conv2d (DepthwiseC (None, 160, 320, 3)       27        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 160, 320, 3)       12        \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 320, 48)      144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160, 320, 48)      192       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 160, 320, 48)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 80, 160, 48)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 80, 160, 48)       432       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 80, 160, 48)       192       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 80, 160, 48)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 160, 96)       4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 80, 160, 96)       384       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 80, 160, 96)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 80, 96)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 40, 80, 96)        864       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40, 80, 96)        384       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 40, 80, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 80, 192)       18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 40, 80, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 40, 80, 192)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 40, 192)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 20, 40, 192)       1728      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 20, 40, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 20, 40, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 40, 384)       73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 40, 384)       1536      \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 20, 40, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 20, 40, 384)       3456      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20, 40, 384)       1536      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 20, 40, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 40, 512)       196608    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 20, 40, 512)       2048      \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 20, 40, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 40, 10)        5120      \n",
      "=================================================================\n",
      "Total params: 312,967\n",
      "Trainable params: 0\n",
      "Non-trainable params: 312,967\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "model_layers = [layer for layer in model.layers]\n",
    "model.summary()\n",
    "cnt = 0\n",
    "bn_weights = []\n",
    "for layer in model_layers:\n",
    "    layer_name = layer.get_config()['name']\n",
    "    if 're_lu' not in layer_name and 'max_pooling2d' not in layer_name:\n",
    "        cnt += 1\n",
    "        if 'batch' in layer_name:\n",
    "            bn_weights.append(weights_list[cnt])\n",
    "        layer.set_weights(weights_list[cnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Correct weights:\n",
    "```\n",
    "[0.8880645155906677, 0.6772263944149017, 0.02124013871572325, 0.058586649582813566]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 160, 320, 3)\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: cuDNN launch failure : input shape ([1,3,160,320])\n\t [[{{node batch_normalization/cond/FusedBatchNorm_1}}]]\n  (1) Internal: cuDNN launch failure : input shape ([1,3,160,320])\n\t [[{{node batch_normalization/cond/FusedBatchNorm_1}}]]\n\t [[conv2d_5/Conv2D/_425]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-898478c907fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'src/images/2.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-pip\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-pip\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-pip\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu-pip\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: cuDNN launch failure : input shape ([1,3,160,320])\n\t [[{{node batch_normalization/cond/FusedBatchNorm_1}}]]\n  (1) Internal: cuDNN launch failure : input shape ([1,3,160,320])\n\t [[{{node batch_normalization/cond/FusedBatchNorm_1}}]]\n\t [[conv2d_5/Conv2D/_425]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "input_img = load_input('src/images/2.jpg')\n",
    "print(input_img.shape)\n",
    "output = model.predict(input_img).transpose(0,3,1,2)\n",
    "print(get_box(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the `Detection` object\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])\n",
    "\n",
    "def bbox_iou(boxA, boxB):\n",
    "    \n",
    "    x_a_ll = boxA[0] - boxA[2]/2\n",
    "    y_a_ll = boxA[1] - boxA[3]/2\n",
    "    x_a_ur = boxA[0] + boxA[2]/2\n",
    "    y_a_ur = boxA[1] + boxA[3]/2\n",
    "    x_b_ll = boxB[0] - boxB[2]/2\n",
    "    y_b_ll = boxB[1] - boxB[3]/2\n",
    "    x_b_ur = boxB[0] + boxB[2]/2\n",
    "    y_b_ur = boxB[1] + boxB[3]/2\n",
    "    \n",
    "    xA = max(x_a_ll, x_b_ll)\n",
    "    yA = max(y_a_ll, y_b_ll)\n",
    "    xB = min(x_a_ur, x_b_ur)\n",
    "    yB = min(y_a_ur, y_b_ur)\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = boxA[2]*boxA[3]\n",
    "    boxBArea = boxB[2]*boxB[3]\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given dataset compute the iou\n",
    "'''\n",
    "import json\n",
    "with open('groundtruth.txt', 'r') as outfile:\n",
    "    lines = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The iou should be about 67%\n",
    "'''\n",
    "avg_iou = 0\n",
    "for line in lines:\n",
    "    input_img = load_input(line[0])\n",
    "    output = model.predict(input_img).transpose(0, 3, 1, 2)\n",
    "    avg_iou += bbox_iou(get_box(output),line[1])\n",
    "avg_iou = avg_iou/len(lines)\n",
    "print(avg_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the image with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = load_input('src/images/2.jpg')\n",
    "output = get_box(model.predict(input_img).transpose(0,3,1,2))\n",
    "output_no_bn = get_box(model_no_bn.predict(input_img).transpose(0,3,1,2))\n",
    "truth = [0.88515625, 0.6763888888888889, 0.0203125, 0.04722222222222222]\n",
    "\n",
    "truth_top_left = (int((truth[0] - truth[2]/2)*640), int((truth[1] + truth[3]/2)*360))\n",
    "truth_bot_right = (int((truth[0] + truth[2]/2)*640), int((truth[1] - truth[3]/2)*360))\n",
    "\n",
    "\n",
    "box_top_left = (int((output[0] - output[2]/2)*640), int((output[1] + output[3]/2)*360))\n",
    "box_bot_right = (int((output[0] + output[2]/2)*640), int((output[1] - output[3]/2)*360))\n",
    "\n",
    "box_no_bn_top_left = (int((output_no_bn[0] - output_no_bn[2]/2)*640), int((output_no_bn[1] + output_no_bn[3]/2)*360))\n",
    "box_no_bn_bot_right = (int((output_no_bn[0] + output_no_bn[2]/2)*640), int((output_no_bn[1] - output_no_bn[3]/2)*360))\n",
    "\n",
    "input_img_1 = Image.open('src/images/2.jpg')\n",
    "input_img_2 = Image.open('src/images/2.jpg')\n",
    "\n",
    "draw1 = ImageDraw.ImageDraw(input_img_1)\n",
    "draw1.rectangle((truth_top_left, truth_bot_right), outline = (255,0,0))\n",
    "draw1.rectangle((box_top_left, box_bot_right), outline = (0,255,0))\n",
    "draw2 = ImageDraw.ImageDraw(input_img_2)\n",
    "draw2.rectangle((truth_top_left, truth_bot_right), outline = (255,0,0))\n",
    "draw2.rectangle((box_no_bn_top_left, box_no_bn_bot_right), outline = (0,255,0))\n",
    "\n",
    "\n",
    "plt.figure(1, figsize = (16, 9), dpi =300)\n",
    "plt.imshow(input_img_1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2, figsize = (16, 9), dpi =300)\n",
    "plt.imshow(input_img_2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
