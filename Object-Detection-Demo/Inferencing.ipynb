{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of an object detector \n",
    "In this notebook, you will need to implement an CNN based object detector to detect specific objects in given images.\n",
    "\n",
    "You will find the resources in folder 'src'. It contains the image set for training and testing, the groundtruth of the images, and a folder to place your parameters (weights).\n",
    "\n",
    "<img src=\"pics/0__1_.jpg\" alt=\"Biker\" />\n",
    "\n",
    "This task is divided into three sections. This notebook contains only the first section. \n",
    "\n",
    "- In the first section, you will load weights to a pre-defined neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the devices available on your machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4991212062333339069\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything, we need to import the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 996 image-label pair in the entire set\n",
      "First entry looks like this:\n",
      "['src/images/932.jpg', [0.48984375, 0.5055555555555555, 0.0328125, 0.08333333333333333]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load groundtruch\n",
    "'''\n",
    "import json\n",
    "with open('src/groundtruth.txt', 'r') as file:\n",
    "    lines = json.load(file)\n",
    "\n",
    "print('There are %d image-label pair in the entire set' %(len(lines)))\n",
    "print('First entry looks like this:')\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to pick ramdomly from the data set to put aside a subset of images for testing. The model **should not** see this subset of images while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function converts the image into the input type.\n",
    "'''\n",
    "def load_input(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((320,160))\n",
    "    input_img = np.asarray(img).astype(np.float32)\n",
    "    input_img = (input_img/255 - 0.5)/0.25\n",
    "    return input_img[np.newaxis,:]\n",
    "\n",
    "'''\n",
    "This function draws the rectangle around the object\n",
    "'''\n",
    "def show_image(image, box):\n",
    "    img = Image.open(image)\n",
    "    \n",
    "    box_top_left = (int((box[0] - box[2]/2)*640), int((box[1] + box[3]/2)*360))\n",
    "    box_bot_right = (int((box[0] + box[2]/2)*640), int((box[1] - box[3]/2)*360))\n",
    "\n",
    "    draw = ImageDraw.ImageDraw(img)\n",
    "    draw.rectangle((box_top_left, box_bot_right), outline = (255,0,0))\n",
    "\n",
    "    plt.figure(1, figsize = (16, 9), dpi =300)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "This is the function to get the predict box (x,y,w,h)\n",
    "'''\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def get_box(output):\n",
    "    anchors = [1.4940052559648322, 2.3598481287086823, 4.0113013115312155, 5.760873975661669]\n",
    "    h = output.shape[2]\n",
    "    w = output.shape[3]\n",
    "    output = output.reshape(2,5,800).transpose(1,0,2).flatten().reshape(5,1600)\n",
    "    grid_x = np.tile(np.tile(np.linspace(0,w-1,w),h).reshape(h,w),(2,1,1)).flatten()\n",
    "    grid_y = np.tile(np.tile(np.linspace(0,h-1,h),w).reshape(w,h).T,(2,1,1)).flatten()\n",
    "    xs = sigmoid(output[0]) + grid_x\n",
    "    ys = sigmoid(output[1]) + grid_y\n",
    "    anchor_w = np.zeros(1600)\n",
    "    anchor_h = np.zeros(1600)\n",
    "    anchor_w[0:800] = anchors[0]\n",
    "    anchor_w[800:1600] = anchors[2]\n",
    "    anchor_h[0:800] = anchors[1]\n",
    "    anchor_h[800:1600] = anchors[3]\n",
    "    ws = np.exp(output[2]) * anchor_w\n",
    "    hs = np.exp(output[3]) * anchor_h\n",
    "    ind = np.argmax(output[4])\n",
    "    bcx = xs[ind]\n",
    "    bcy = ys[ind]\n",
    "    bw = ws[ind]\n",
    "    bh = hs[ind]\n",
    "    box = [bcx/w, bcy/h, bw/w, bh/h]\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('src/params_sets_alt', 'rb') as fp:\n",
    "    weights_list = pickle.load(fp, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section 1\n",
    "First, a pre-defined keras model is given. Your task is to play with the training configurations and explore their effects on the training performance (speed, accuracy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aperture\\Anaconda3\\envs\\tf-conda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#this is the model for the testing part\n",
    "model = tf.keras.models.Sequential([\n",
    "    #first dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False, input_shape=(160, 320, 3)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(48, (1, 1), padding='same', use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #maxpooling\n",
    "    layers.MaxPool2D(strides =(2, 2)),\n",
    "    #second dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(96,(1,1), padding='same', use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #maxpooling\n",
    "    layers.MaxPool2D(strides =(2, 2)),\n",
    "    #third dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(192, (1, 1), padding='same', use_bias=False,strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #maxpooling\n",
    "    layers.MaxPool2D(strides=(2, 2)),\n",
    "    #fourth dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(384, (1, 1), padding='same', use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #fifth dw module\n",
    "    layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1, strides=(1, 1), use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    layers.Conv2D(512, (1, 1), padding='same',use_bias=False, strides=(1, 1)),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=1e-5, trainable=False),\n",
    "    layers.ReLU(4.0),\n",
    "    #output\n",
    "    layers.Conv2D(10,(1,1), padding='same',use_bias=False,strides=(1, 1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "depthwise_conv2d (DepthwiseC (None, 160, 320, 3)       27        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 160, 320, 3)       12        \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 320, 48)      144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 160, 320, 48)      192       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 160, 320, 48)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 80, 160, 48)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 80, 160, 48)       432       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 80, 160, 48)       192       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 80, 160, 48)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 160, 96)       4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 80, 160, 96)       384       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 80, 160, 96)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 80, 96)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 40, 80, 96)        864       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 40, 80, 96)        384       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 40, 80, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 80, 192)       18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 40, 80, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 40, 80, 192)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 40, 192)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 20, 40, 192)       1728      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 20, 40, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 20, 40, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 40, 384)       73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 20, 40, 384)       1536      \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 20, 40, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 20, 40, 384)       3456      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 20, 40, 384)       1536      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 20, 40, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 40, 512)       196608    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 20, 40, 512)       2048      \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 20, 40, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 40, 10)        5120      \n",
      "=================================================================\n",
      "Total params: 312,967\n",
      "Trainable params: 0\n",
      "Non-trainable params: 312,967\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "model_layers = [layer for layer in model.layers]\n",
    "model.summary()\n",
    "cnt = 0\n",
    "bn_weights = []\n",
    "for layer in model_layers:\n",
    "    layer_name = layer.get_config()['name']\n",
    "    if 're_lu' not in layer_name and 'max_pooling2d' not in layer_name:\n",
    "        cnt += 1\n",
    "        if 'batch' in layer_name:\n",
    "            bn_weights.append(weights_list[cnt])\n",
    "        layer.set_weights(weights_list[cnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Correct weights:\n",
    "```\n",
    "[0.8880645155906677, 0.6772263944149017, 0.02124013871572325, 0.058586649582813566]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 160, 320, 3)\n",
      "(1, 10, 20, 40)\n",
      "[0.8880645126104355, 0.6772263884544373, 0.021240140941964566, 0.05858664254991809]\n",
      "(1600,)\n"
     ]
    }
   ],
   "source": [
    "input_img = load_input('src/images/2.jpg')\n",
    "print(input_img.shape)\n",
    "output = model.predict(input_img).transpose(0,3,1,2)\n",
    "print(output.shape)\n",
    "print(get_box(output))\n",
    "output = output.reshape(2,5,800).transpose(1,0,2).flatten().reshape(5,1600)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the `Detection` object\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])\n",
    "\n",
    "def bbox_iou(boxA, boxB):\n",
    "    \n",
    "    x_a_ll = boxA[0] - boxA[2]/2\n",
    "    y_a_ll = boxA[1] - boxA[3]/2\n",
    "    x_a_ur = boxA[0] + boxA[2]/2\n",
    "    y_a_ur = boxA[1] + boxA[3]/2\n",
    "    x_b_ll = boxB[0] - boxB[2]/2\n",
    "    y_b_ll = boxB[1] - boxB[3]/2\n",
    "    x_b_ur = boxB[0] + boxB[2]/2\n",
    "    y_b_ur = boxB[1] + boxB[3]/2\n",
    "    \n",
    "    xA = max(x_a_ll, x_b_ll)\n",
    "    yA = max(y_a_ll, y_b_ll)\n",
    "    xB = min(x_a_ur, x_b_ur)\n",
    "    yB = min(y_a_ur, y_b_ur)\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = boxA[2]*boxA[3]\n",
    "    boxBArea = boxB[2]*boxB[3]\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given dataset compute the iou\n",
    "'''\n",
    "import json\n",
    "with open('src/groundtruth.txt', 'r') as outfile:\n",
    "    lines = json.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On CPU (i7-8850H):\n",
    "\n",
    "- Average IoU = 0.67594\n",
    "- Average FPS = 14.94236\n",
    "\n",
    "On NVIDIA P1000:\n",
    "\n",
    "- Average IoU = 0.67594\n",
    "- Average FPS = 93.60269\n",
    "\n",
    "On NVIDIA P2000:\n",
    "\n",
    "- Average IoU = 0.67594\n",
    "- Average FPS = 111.15506\n",
    "\n",
    "On NVIDIA Jetson Xavier\n",
    " \n",
    "- Average IoU = 0.67734\n",
    "- Average FPS = 32.00590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bb0c06e0d9cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtime_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtime_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-conda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[1;32m-> 1113\u001b[1;33m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-conda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-conda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "The iou should be about 67%\n",
    "'''\n",
    "total_iou = 0\n",
    "total_time = 0\n",
    "for line in lines:\n",
    "    input_img = load_input(line[0])\n",
    "    \n",
    "    time_s = time.perf_counter()\n",
    "    output = model.predict(input_img).transpose(0, 3, 1, 2)\n",
    "    time_t = time.perf_counter()\n",
    "    \n",
    "    total_iou += bbox_iou(get_box(output),line[1])\n",
    "    total_time += time_t - time_s\n",
    "avg_iou = total_iou/len(lines)\n",
    "avg_time = total_time/len(lines)\n",
    "print('Average IoU = %.5f' %(avg_iou))\n",
    "print('Average FPS = %.5f' %(1/avg_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the image with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = load_input('src/images/2.jpg')\n",
    "output = get_box(model.predict(input_img).transpose(0,3,1,2))\n",
    "truth = [0.88515625, 0.6763888888888889, 0.0203125, 0.04722222222222222]\n",
    "\n",
    "truth_top_left = (int((truth[0] - truth[2]/2)*640), int((truth[1] + truth[3]/2)*360))\n",
    "truth_bot_right = (int((truth[0] + truth[2]/2)*640), int((truth[1] - truth[3]/2)*360))\n",
    "\n",
    "box_top_left = (int((output[0] - output[2]/2)*640), int((output[1] + output[3]/2)*360))\n",
    "box_bot_right = (int((output[0] + output[2]/2)*640), int((output[1] - output[3]/2)*360))\n",
    "\n",
    "input_img = Image.open('src/images/2.jpg')\n",
    "\n",
    "draw = ImageDraw.ImageDraw(input_img)\n",
    "draw.rectangle((truth_top_left, truth_bot_right), outline = (255,0,0))\n",
    "draw.rectangle((box_top_left, box_bot_right), outline = (0,255,0))\n",
    "\n",
    "\n",
    "plt.figure(1, figsize = (16, 9), dpi =300)\n",
    "plt.imshow(input_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
